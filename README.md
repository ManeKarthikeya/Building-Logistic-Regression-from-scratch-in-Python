GitHub Markdown supports LaTeX math rendering **only through GitHub's math rendering feature** which is enabled by wrapping equations in `$$` or `$`. However, this requires:

1. **Using `$$` for block equations** or `$` for inline
2. **GitHub's Markdown processor must support it** (it does for most repositories)

Here's the corrected version with proper GitHub Markdown math formatting:

---

# Logistic Regression from Scratch in Python

## ðŸ“Œ Overview
This project implements a **Logistic Regression classifier from scratch** using only NumPy, without relying on high-level machine learning libraries like scikit-learn. The model is trained and tested on the PIMA Diabetes Dataset.

## ðŸ“– Theory Summary

### 1. Hypothesis (Sigmoid Function)
$$
h_\theta(x) = \sigma(z) = \frac{1}{1 + e^{-z}}, \quad z = \theta^T x
$$

### 2. Cost Function (Log Loss)
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i)) \right]
$$

### 3. Gradient Descent Update Rule
$$
\theta_j := \theta_j - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_j}
$$

### 4. Gradient Derivative
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) \cdot x_{ij}
$$

### 5. Vectorized Update Formula
$$
\theta := \theta - \alpha \cdot \frac{1}{m} \cdot X^T (h - y)
$$

### 6. Learning Rate
$$
0 < \alpha \le 1 \quad (\text{Typical values: } 0.01, 0.001, 0.1)
$$

### 7. Sigmoid Derivative
$$
\sigma'(z) = \sigma(z)(1 - \sigma(z))
$$

---

**Alternative (if GitHub doesn't render the LaTeX):**

If GitHub doesn't render the math properly, you can use plain text with Unicode characters:

## ðŸ“– Theory Summary

### 1. Hypothesis (Sigmoid Function)
```
h_Î¸(x) = Ïƒ(z) = 1 / (1 + e^(-z))
where z = Î¸áµ€x
```

### 2. Cost Function (Log Loss)
```
J(Î¸) = -(1/m) * Î£ [ yáµ¢ log(h_Î¸(xáµ¢)) + (1 âˆ’ yáµ¢) log(1 âˆ’ h_Î¸(xáµ¢)) ]
```

### 3. Gradient Descent Update Rule
```
Î¸â±¼ := Î¸â±¼ âˆ’ Î± * âˆ‚J(Î¸)/âˆ‚Î¸â±¼
```

### 4. Gradient Derivative
```
âˆ‚J(Î¸)/âˆ‚Î¸â±¼ = (1/m) * Î£ (h_Î¸(xáµ¢) âˆ’ yáµ¢) * xáµ¢â±¼
```

### 5. Vectorized Update Formula
```
Î¸ := Î¸ âˆ’ Î± * (1/m) * Xáµ€ (h âˆ’ y)
```

### 6. Learning Rate
```
0 < Î± â‰¤ 1  (Typical values: 0.01, 0.001, 0.1)
```

### 7. Sigmoid Derivative
```
Ïƒâ€²(z) = Ïƒ(z)(1 âˆ’ Ïƒ(z))
```

---

## ðŸ§  Implementation Details

### Class: `Logistic_Regression`
- **Initialization**: Sets learning rate and number of iterations.
- **fit(X, Y)**: Trains the model using gradient descent.
- **update_weight()**: Performs weight and bias updates.
- **predict(X)**: Returns binary predictions based on a 0.5 threshold.

### Key Steps:
1. **Data Preprocessing**: Standardization using `StandardScaler`.
2. **Train-Test Split**: 80% training, 20% testing.
3. **Model Training**: Gradient descent optimization.
4. **Evaluation**: Accuracy score on training and test sets.
5. **Prediction**: Example prediction for a new data point.

---

## ðŸ“Š Dataset: PIMA Diabetes
- **Rows**: 768
- **Columns**: 8 features + 1 target (`Outcome`)
- **Target**:
  - `0` â†’ Non-Diabetic
  - `1` â†’ Diabetic

---

## ðŸš€ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/logistic-regression-from-scratch.git
cd logistic-regression-from-scratch
```

### 2. Install dependencies
```bash
pip install numpy pandas scikit-learn
```

### 3. Run the script
Ensure `diabetes.csv` is in the same directory or update the path in the script.

---

## ðŸ“ˆ Results
- **Training Accuracy**: ~79%
- **Test Accuracy**: ~75%

Example prediction:
```
Input: (6,148,72,35,0,33.6,0.627,50)
Output: The person is diabetic
```

---

## ðŸ› ï¸ Dependencies
- NumPy
- Pandas
- Scikit-learn (for preprocessing and evaluation only)

---

## ðŸ“ Files
- `logistic_regression_from_scratch.py` â€“ Main implementation
- `diabetes.csv` â€“ Dataset (not included in repo, download from [Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database))
- `README.md` â€“ This file

---

## ðŸ“Œ Key Learnings
- Understanding the math behind logistic regression
- Implementing gradient descent manually
- Preprocessing data for ML models
- Evaluating classification performance

---

*This project is for educational purposes to demonstrate the inner workings of logistic regression.*

---
