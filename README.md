# Logistic Regression from Scratch in Python

## ğŸ“Œ Overview
This project implements a **Logistic Regression classifier from scratch** using only NumPy, without relying on high-level machine learning libraries like scikit-learn. The model is trained and tested on the PIMA Diabetes Dataset.

## ğŸ“– Theory Summary

1. Hypothesis (Sigmoid Function)
h
Î¸
(
x
)
=
Ïƒ
(
z
)
=
1
1
+
e
âˆ’
z
,
z
=
Î¸
T
x
h 
Î¸
â€‹
 (x)=Ïƒ(z)= 
1+e 
âˆ’z
 
1
â€‹
 ,z=Î¸ 
T
 x
2. Cost Function (Log Loss)
J
(
Î¸
)
=
âˆ’
1
m
âˆ‘
i
=
1
m
[
y
i
log
â¡
(
h
Î¸
(
x
i
)
)
+
(
1
âˆ’
y
i
)
log
â¡
(
1
âˆ’
h
Î¸
(
x
i
)
)
]
J(Î¸)=âˆ’ 
m
1
â€‹
  
i=1
âˆ‘
m
â€‹
 [y 
i
â€‹
 log(h 
Î¸
â€‹
 (x 
i
â€‹
 ))+(1âˆ’y 
i
â€‹
 )log(1âˆ’h 
Î¸
â€‹
 (x 
i
â€‹
 ))]
3. Gradient Descent Update Rule
Î¸
j
:
=
Î¸
j
âˆ’
Î±
â‹…
âˆ‚
J
(
Î¸
)
âˆ‚
Î¸
j
Î¸ 
j
â€‹
 :=Î¸ 
j
â€‹
 âˆ’Î±â‹… 
âˆ‚Î¸ 
j
â€‹
 
âˆ‚J(Î¸)
â€‹
 
4. Gradient Derivative
âˆ‚
J
(
Î¸
)
âˆ‚
Î¸
j
=
1
m
âˆ‘
i
=
1
m
(
h
Î¸
(
x
i
)
âˆ’
y
i
)
â‹…
x
i
j
âˆ‚Î¸ 
j
â€‹
 
âˆ‚J(Î¸)
â€‹
 = 
m
1
â€‹
  
i=1
âˆ‘
m
â€‹
 (h 
Î¸
â€‹
 (x 
i
â€‹
 )âˆ’y 
i
â€‹
 )â‹…x 
ij
â€‹
 
5. Vectorized Update Formula
Î¸
:
=
Î¸
âˆ’
Î±
â‹…
1
m
â‹…
X
T
(
h
âˆ’
y
)
Î¸:=Î¸âˆ’Î±â‹… 
m
1
â€‹
 â‹…X 
T
 (hâˆ’y)
6. Learning Rate
0
<
Î±
â‰¤
1
(
Typical values: 
0.01
,
0.001
,
0.1
)
0<Î±â‰¤1(Typical values: 0.01,0.001,0.1)
7. Sigmoid Derivative
Ïƒ
â€²
(
z
)
=
Ïƒ
(
z
)
(
1
âˆ’
Ïƒ
(
z
)
)
Ïƒ 
â€²
 (z)=Ïƒ(z)(1âˆ’Ïƒ(z))

### 1. Hypothesis (Sigmoid Function)
\[
h_\theta(x) = \sigma(z) = \frac{1}{1 + e^{-z}}, \quad z = \theta^T x
\]

### 2. Cost Function (Log Loss)
\[
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i)) \right]
\]

### 3. Gradient Descent Update Rule
\[
\theta_j := \theta_j - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_j}
\]

### 4. Gradient Derivative
\[
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i) - y_i) \cdot x_{ij}
\]

### 5. Vectorized Update Formula
\[
\theta := \theta - \alpha \cdot \frac{1}{m} \cdot X^T (h - y)
\]

### 6. Learning Rate
\[
0 < \alpha \le 1 \quad (\text{Typical values: } 0.01, 0.001, 0.1)
\]

### 7. Sigmoid Derivative
\[
\sigma'(z) = \sigma(z)(1 - \sigma(z))
\]

---

## ğŸ§  Implementation Details

### Class: `Logistic_Regression`
- **Initialization**: Sets learning rate and number of iterations.
- **fit(X, Y)**: Trains the model using gradient descent.
- **update_weight()**: Performs weight and bias updates.
- **predict(X)**: Returns binary predictions based on a 0.5 threshold.

### Key Steps:
1. **Data Preprocessing**: Standardization using `StandardScaler`.
2. **Train-Test Split**: 80% training, 20% testing.
3. **Model Training**: Gradient descent optimization.
4. **Evaluation**: Accuracy score on training and test sets.
5. **Prediction**: Example prediction for a new data point.

---

## ğŸ“Š Dataset: PIMA Diabetes
- **Rows**: 768
- **Columns**: 8 features + 1 target (`Outcome`)
- **Target**:
  - `0` â†’ Non-Diabetic
  - `1` â†’ Diabetic

---

## ğŸš€ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/logistic-regression-from-scratch.git
cd logistic-regression-from-scratch
```

### 2. Install dependencies
```bash
pip install numpy pandas scikit-learn
```

### 3. Run the script
Ensure `diabetes.csv` is in the same directory or update the path in the script.

---

## ğŸ“ˆ Results
- **Training Accuracy**: ~79%
- **Test Accuracy**: ~75%

Example prediction:
```
Input: (6,148,72,35,0,33.6,0.627,50)
Output: The person is diabetic
```

---

## ğŸ› ï¸ Dependencies
- NumPy
- Pandas
- Scikit-learn (for preprocessing and evaluation only)

---

## ğŸ“ Files
- `logistic_regression_from_scratch.py` â€“ Main implementation
- `diabetes.csv` â€“ Dataset (included in repo, download from [Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database))
- `README.md` â€“ This file

---

## ğŸ“Œ Key Learnings
- Understanding the math behind logistic regression
- Implementing gradient descent manually
- Preprocessing data for ML models
- Evaluating classification performance

---

*This project is for educational purposes to demonstrate the inner workings of logistic regression.*
