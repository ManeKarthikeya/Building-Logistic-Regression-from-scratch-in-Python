# -*- coding: utf-8 -*-
"""Building Logistic Regression from scratch in Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14F6SRqM7DESizUcPhWu7kGrFlzuLYURb

---

## 1. Logistic Regression Formula

### Hypothesis (Sigmoid Function)

```text
h_θ(x) = σ(z) = 1 / (1 + e^(−z))
where z = θᵀx
```

Expanded:

```text
h_θ(x) = 1 / (1 + e^(−(θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ)))
```

---

## 2. Cost Function (Log Loss)

For **m training examples**:

```text
J(θ) = −(1/m) * Σ [ yᵢ log(h_θ(xᵢ)) + (1 − yᵢ) log(1 − h_θ(xᵢ)) ]
```

---

## 3. Gradient Descent Update Rule

```text
θⱼ := θⱼ − α * ∂J(θ)/∂θⱼ
```

Where:

* `α` = learning rate
* `j = 0, 1, 2, ..., n`

---

## 4. Derivative of the Cost Function

For Logistic Regression:

```text
∂J(θ)/∂θⱼ = (1/m) * Σ (h_θ(xᵢ) − yᵢ) * xᵢⱼ
```

---

## 5. Final Gradient Descent Formula (Vectorized)

```text
θ := θ − α * (1/m) * Xᵀ (h − y)
```

Where:

* `X` = feature matrix
* `h = σ(Xθ)`
* `y` = target vector

---

## 6. Learning Rate (α)

```text
0 < α ≤ 1
```

Typical values:

```text
α = 0.01, 0.001, 0.1
```

---

## 7. Sigmoid Derivative (Often Asked)

```text
σ′(z) = σ(z)(1 − σ(z))
```

---

## 8. All-in-One (Compact Summary)

```text
h = σ(Xθ)
J(θ) = −(1/m)[yᵀlog(h) + (1 − y)ᵀlog(1 − h)]
θ := θ − α(1/m)Xᵀ(h − y)
```

---

Importing the Dependencies
"""

# importing numpy library
import numpy as np

"""**Logistic Regression**"""

class Logistic_Regression():

  # declaring learning rate & number of iterations (Hyperparametes)
  def __init__(self, learning_rate, no_of_iterations):

    self.learning_rate = learning_rate
    self.no_of_iterations = no_of_iterations

  # fit function to train the model with dataset
  def fit(self, X, Y):

    #number of data points in the dataset (number of rows) --> m
    #number of input features in the dataset (number of columns) --> n
    self.m, self.n = X.shape

    # Initiating the weight & bias value

    self.w = np.zeros(self.n)
    self.b = 0
    self.X = X
    self.Y = Y

    # implementing Gradient Decent for Optimization
    for i in range(self.no_of_iterations):
      self.update_weight()

  # function to update the weight & bias value


  def update_weight(self):

    # Y_hat formula (sigmoid function)
    Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + (self.b))))

    # deravities
    dw = (1/self.m) * np.dot(self.X.T, (Y_hat - self.Y))
    db = (1/self.m) * np.sum(Y_hat - self.Y)

    # updating weights & bias using Gradient descent
    self.w = self.w - self.learning_rate * dw
    self.b = self.b - self.learning_rate * db

  # Sigmoid Function & Decision Boundry
  def predict(self, X):

    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + (self.b))))
    Y_pred = np.where(Y_pred > 0.5, 1, 0)
    return Y_pred

"""Importing Dependencies"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

"""Data Collection & Analysis

PIMA Diabetes Dataset
"""

# loading the diabetes to a pandas DataFrame
diabetes_dataset = pd.read_csv('/content/diabetes.csv')

#printing the first 5 rows of Dataset
diabetes_dataset.head()

# Number of rows & columns
diabetes_dataset.shape

# getting the statistical measures of data
diabetes_dataset.describe()

diabetes_dataset['Outcome'].value_counts()

"""0 --> Non-Diabetic

1 --> Diabetic
"""

diabetes_dataset.groupby('Outcome').mean()

# Separating the data & labels
features = diabetes_dataset.drop(columns = 'Outcome', axis = 1)
target = diabetes_dataset['Outcome']

print(features)

print(target)

"""Data Standardization"""

scaler = StandardScaler()

scaler.fit(features)

standardized_data = scaler.transform(features)

print(standardized_data)

features = standardized_data
target = diabetes_dataset['Outcome']

print(features)
print(target)

"""Train Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = 2)

print(features.shape, X_train.shape, X_test.shape)

"""Training Model"""

classifier = Logistic_Regression(learning_rate = 0.01, no_of_iterations = 1000)

# training the support vactor mechane classifier
classifier.fit(X_train, Y_train)

"""***Model Evaluation***

Accuracy Score
"""

# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

print('Accuracy score of the traning data : ', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)

print('Accuracy score of the test data : ', test_data_accuracy)

"""Making a Presictive System"""

input_data = (6,148,72,35,0,33.6,0.627,50)

# changing input data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standarsize input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
    print('The person is diabetic')

